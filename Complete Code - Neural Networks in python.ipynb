{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "obvious-looking",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.loadtxt('train_X.csv', delimiter = ',').T\n",
    "Y_train = np.loadtxt('train_label.csv', delimiter = ',').T\n",
    "\n",
    "X_test = np.loadtxt('test_X.csv', delimiter = ',').T\n",
    "Y_test = np.loadtxt('test_label.csv', delimiter = ',').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of X_train :\", X_train.shape)\n",
    "print(\"shape of Y_train :\", Y_train.shape)\n",
    "print(\"shape of X_test :\", X_test.shape)\n",
    "print(\"shape of Y_test :\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.randrange(0, X_train.shape[1])\n",
    "plt.imshow(X_train[:, index].reshape(28, 28), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-wagner",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "![nn](Images/nn.png)\n",
    "\n",
    "## Initialize parameters Randomly\n",
    "$ W_1 = np.random.randn(n_1, n_0) $\n",
    "\n",
    "$ b_1 = np.zeros((n_1, 1))$\n",
    "\n",
    "$ W_2 = np.random.randn(n_2, n_1) $\n",
    "\n",
    "$ b_2 = np.zeros((n_2, 1))$\n",
    "\n",
    "\n",
    "## *Repeat Below Steps for many times : *\n",
    "\n",
    "\n",
    "## Forward Propagation \n",
    "\n",
    "$ Z_1 = W_1 * X + B_1 $\n",
    "\n",
    "$ A_1 = f ( Z_1 ) $  \n",
    "\n",
    "$ Z_2 = W2 * A_1 + B_2 $\n",
    "\n",
    "$ A_2 = Softmax( Z_2 ) $\n",
    "\n",
    "## Softmax \n",
    "\n",
    "$ a_i = \\frac{e^{z_i}}{\\sum_{i=k}^ne^{z_k}}$\n",
    "\n",
    "\n",
    "## Cost Function \n",
    "\n",
    "$Loss = - \\sum_{i=k}^{n}[ y_k*log(a_k) ]$\n",
    "\n",
    "$Cost = - \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n}[ y_k*log(a_k) ]$\n",
    "\n",
    "\n",
    "\n",
    "## Backward Propagation\n",
    "$dZ_2 = ( A_2 - Y )$\n",
    "\n",
    "$ dW_2 = \\frac{1}{m}. dZ_2 . A_1^T$\n",
    "\n",
    "$ dB_2 = \\frac{1}{m}.sum(dZ_2, 1)$\n",
    "\n",
    "\n",
    "\n",
    "$dZ_1 = W_2^T . dZ_2 * f_1^|(Z_1) $\n",
    "\n",
    "$dW_1 = \\frac{1}{m}.dZ_1.X^T$\n",
    "\n",
    "$dB_1 = \\frac{1}{m}.sum(dZ_1, 1)$\n",
    "\n",
    "\n",
    "## Updating Parameters\n",
    "\n",
    "$ W_2 = W_2 -  \\alpha * \\frac{\\partial Cost }{\\partial W_2}$ \n",
    "\n",
    "$ B_2 = B_2 -  \\alpha * \\frac{\\partial Cost }{\\partial B_2}$ \n",
    "\n",
    "$ W_1 = W_1 -  \\alpha * \\frac{\\partial Cost }{\\partial W_1}$ \n",
    "\n",
    "$ B_1 = B_1 -  \\alpha * \\frac{\\partial Cost }{\\partial B_1}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(x):\n",
    "    expX = np.exp(x)\n",
    "    return expX/np.sum(expX, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(np.tanh(x), 2))\n",
    "\n",
    "def derivative_relu(x):\n",
    "    return np.array(x > 0, dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-collar",
   "metadata": {},
   "source": [
    "# Initialize Parameters\n",
    "\n",
    "### *Note I multiplied 0.01 with weights W. But, this is still large, and that is the reason we saw increase in Cost value at the beginning, while training the model with ReLU function.*\n",
    "\n",
    "### *So, instead, you can multiply with 0.001, and it will solve the problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    w1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    \n",
    "    w2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1,\n",
    "        \"b1\" : b1,\n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-thailand",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "$ Z_1 = W_1 * X + B_1 $\n",
    "\n",
    "$ A_1 = f ( Z_1 ) $  \n",
    "\n",
    "$ Z_2 = W2 * A_1 + B_2 $\n",
    "\n",
    "$ A_2 = Softmax( Z_2 ) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, parameters):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    z1 = np.dot(w1, x) + b1\n",
    "    a1 = tanh(z1)\n",
    "    \n",
    "    z2 = np.dot(w2, a1) + b2\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    forward_cache = {\n",
    "        \"z1\" : z1,\n",
    "        \"a1\" : a1,\n",
    "        \"z2\" : z2,\n",
    "        \"a2\" : a2\n",
    "    }\n",
    "    \n",
    "    return forward_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-charger",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "$Cost = - \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{n}[ y_k*log(a_k) ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(a2, y):\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    cost = -(1/m)*np.sum(y*np.log(a2))\n",
    "    \n",
    "    #cost = -(1/m)*np.sum(np.sum(y*np.log(a2, 0), 1))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-easter",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "$dZ_2 = ( A_2 - Y )$\n",
    "\n",
    "$ dW_2 = \\frac{1}{m}. dZ_2 . A_1^T$\n",
    "\n",
    "$ dB_2 = \\frac{1}{m}.sum(dZ_2, 1)$\n",
    "\n",
    "\n",
    "\n",
    "$dZ_1 = W_2^T . dZ_2 * f_1^|(Z_1) $\n",
    "\n",
    "$dW_1 = \\frac{1}{m}.dZ_1.X^T$\n",
    "\n",
    "$dB_1 = \\frac{1}{m}.sum(dZ_1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x, y, parameters, forward_cache):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    a1 = forward_cache['a1']\n",
    "    a2 = forward_cache['a2']\n",
    "    \n",
    "    m = x.shape[1]\n",
    "    \n",
    "    dz2 = (a2 - y)\n",
    "    dw2 = (1/m)*np.dot(dz2, a1.T)\n",
    "    db2 = (1/m)*np.sum(dz2, axis = 1, keepdims = True)\n",
    "    \n",
    "    dz1 = (1/m)*np.dot(w2.T, dz2)*derivative_tanh(a1)\n",
    "    dw1 = (1/m)*np.dot(dz1, x.T)\n",
    "    db1 = (1/m)*np.sum(dz1, axis = 1, keepdims = True)\n",
    "    \n",
    "    gradients = {\n",
    "        \"dw1\" : dw1,\n",
    "        \"db1\" : db1,\n",
    "        \"dw2\" : dw2,\n",
    "        \"db2\" : db2\n",
    "    }\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-hostel",
   "metadata": {},
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \n",
    "    w1 = parameters['w1']\n",
    "    b1 = parameters['b1']\n",
    "    w2 = parameters['w2']\n",
    "    b2 = parameters['b2']\n",
    "    \n",
    "    dw1 = gradients['dw1']\n",
    "    db1 = gradients['db1']\n",
    "    dw2 = gradients['dw2']\n",
    "    db2 = gradients['db2']\n",
    "    \n",
    "    w1 = w1 - learning_rate*dw1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    w2 = w2 - learning_rate*dw2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    \n",
    "    parameters = {\n",
    "        \"w1\" : w1,\n",
    "        \"b1\" : b1,\n",
    "        \"w2\" : w2,\n",
    "        \"b2\" : b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-julian",
   "metadata": {},
   "source": [
    "# Complete Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dietary-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x, y, n_h, learning_rate, iterations):\n",
    "    \n",
    "    n_x = x.shape[0]\n",
    "    n_y = y.shape[0]\n",
    "    \n",
    "    cost_list = []\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        \n",
    "        forward_cache = forward_propagation(x, parameters)\n",
    "        \n",
    "        cost = cost_function(forward_cache['a2'], y)\n",
    "        \n",
    "        gradients = backward_prop(x, y, parameters, forward_cache)\n",
    "        \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        cost_list.append(cost)\n",
    "        \n",
    "        if(i%(iterations/10) == 0):\n",
    "            print(\"Cost after\", i, \"iterations is :\", cost)\n",
    "        \n",
    "    return parameters, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "n_h = 1000\n",
    "learning_rate = 0.02\n",
    "Parameters, Cost_list = model(X_train, Y_train, n_h = n_h, learning_rate = learning_rate, iterations = iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(0, iterations)\n",
    "plt.plot(t, Cost_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(inp, labels, parameters):\n",
    "    forward_cache = forward_propagation(inp, parameters)\n",
    "    a_out = forward_cache['a2']   # containes propabilities with shape(10, 1)\n",
    "    \n",
    "    a_out = np.argmax(a_out, 0)  # 0 represents row wise \n",
    "    \n",
    "    labels = np.argmax(labels, 0)\n",
    "    \n",
    "    acc = np.mean(a_out == labels)*100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of Train Dataset\", accuracy(X_train, Y_train, Parameters), \"%\")\n",
    "print(\"Accuracy of Test Dataset\", round(accuracy(X_test, Y_test, Parameters), 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = int(random.randrange(0,X_test.shape[1]))\n",
    "plt.imshow(X_test[:, idx].reshape((28,28)),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "cache = forward_prop(X_test[:, idx].reshape(X_test[:, idx].shape[0], 1), Parameters)\n",
    "a_pred = cache['a2']  \n",
    "a_pred = np.argmax(a_pred, 0)\n",
    "\n",
    "print(\"Our model says it is :\", a_pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
